0: n7
0: /home/richardalbr/computer_vision/CLTforSemanticSegmentation
0: Sa 10. Dez 02:14:54 KST 2022
absl-py==1.3.0
addict==2.4.0
aiohttp==3.8.3
aiosignal==1.3.1
async-timeout==4.0.2
attrs==22.1.0
cachetools==5.2.0
certifi==2022.9.24
charset-normalizer==2.1.1
click==8.1.3
colorama==0.4.6
commonmark==0.9.1
contourpy==1.0.6
cycler==0.11.0
fonttools==4.38.0
frozenlist==1.3.3
fsspec==2022.11.0
future==0.18.2
google-auth==2.15.0
google-auth-oauthlib==0.4.6
grpcio==1.51.1
idna==3.4
importlib-metadata==5.1.0
kiwisolver==1.4.4
lightning-utilities==0.4.2
Markdown==3.4.1
MarkupSafe==2.1.1
matplotlib==3.6.2
mmcls==0.24.1
mmcv-full==1.6.0
model-index==0.1.11
multidict==6.0.3
numpy==1.23.5
oauthlib==3.2.2
opencv-python==4.6.0.66
openmim==0.3.3
ordered-set==4.1.0
packaging==21.3
pandas==1.5.2
Pillow==9.3.0
prettytable==3.5.0
protobuf==3.20.1
pyasn1==0.4.8
pyasn1-modules==0.2.8
Pygments==2.13.0
pyparsing==3.0.9
python-dateutil==2.8.2
pytorch-lightning==1.8.4
pytz==2022.6
PyYAML==6.0
requests==2.28.1
requests-oauthlib==1.3.1
rich==12.6.0
rsa==4.9
six==1.16.0
tabulate==0.9.0
tensorboard==2.11.0
tensorboard-data-server==0.6.1
tensorboard-plugin-wit==1.8.1
tensorboardX==2.5.1
torch==1.12.1+cu113
torchaudio==0.12.1+cu113
torchmetrics==0.11.0
torchvision==0.13.1+cu113
tqdm==4.64.1
typing_extensions==4.4.0
urllib3==1.26.13
wcwidth==0.2.5
Werkzeug==2.2.2
yapf==0.32.0
yarl==1.8.2
zipp==3.11.0
0,1
2022-12-10 02:15:10,465 - mmseg - INFO - Loaded 1464 images
/home/richardalbr/computer_vision/CLTforSemanticSegmentation/mmsegmentation/mmseg/models/backbones/vit.py:219: UserWarning: DeprecationWarning: pretrained is deprecated, please use "init_cfg" instead
  warnings.warn('DeprecationWarning: pretrained is deprecated, '
/home/richardalbr/computer_vision/CLTforSemanticSegmentation/mmsegmentation/mmseg/models/losses/cross_entropy_loss.py:235: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.
  warnings.warn(
/home/richardalbr/computer_vision/cvvenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2022-12-10 02:15:18,529 - mmseg - INFO - Loaded 1449 images
2022-12-10 02:15:18,529 - mmseg - INFO - load checkpoint from local path: /home/richardalbr/computer_vision/CLTforSemanticSegmentation/segmenter_vit-t_mask_8x1_512x512_160k_ade20k_20220105_151706-ffcf7509.pth
2022-12-10 02:15:18,582 - mmseg - WARNING - The model and loaded state dict do not match exactly

size mismatch for decode_head.cls_emb: copying a param with shape torch.Size([1, 150, 192]) from checkpoint, the shape in current model is torch.Size([1, 21, 192]).
size mismatch for decode_head.mask_norm.weight: copying a param with shape torch.Size([150]) from checkpoint, the shape in current model is torch.Size([21]).
size mismatch for decode_head.mask_norm.bias: copying a param with shape torch.Size([150]) from checkpoint, the shape in current model is torch.Size([21]).
missing keys in source state_dict: auxiliary_head.conv_seg.weight, auxiliary_head.conv_seg.bias, auxiliary_head.transformer.backbone.cls_token, auxiliary_head.transformer.backbone.pos_embed, auxiliary_head.transformer.backbone.patch_embed.projection.weight, auxiliary_head.transformer.backbone.patch_embed.projection.bias, auxiliary_head.transformer.backbone.layers.0.ln1.weight, auxiliary_head.transformer.backbone.layers.0.ln1.bias, auxiliary_head.transformer.backbone.layers.0.attn.attn.in_proj_weight, auxiliary_head.transformer.backbone.layers.0.attn.attn.in_proj_bias, auxiliary_head.transformer.backbone.layers.0.attn.attn.out_proj.weight, auxiliary_head.transformer.backbone.layers.0.attn.attn.out_proj.bias, auxiliary_head.transformer.backbone.layers.0.ln2.weight, auxiliary_head.transformer.backbone.layers.0.ln2.bias, auxiliary_head.transformer.backbone.layers.0.ffn.layers.0.0.weight, auxiliary_head.transformer.backbone.layers.0.ffn.layers.0.0.bias, auxiliary_head.transformer.backbone.layers.0.ffn.layers.1.weight, auxiliary_head.transformer.backbone.layers.0.ffn.layers.1.bias, auxiliary_head.transformer.backbone.layers.1.ln1.weight, auxiliary_head.transformer.backbone.layers.1.ln1.bias, auxiliary_head.transformer.backbone.layers.1.attn.attn.in_proj_weight, auxiliary_head.transformer.backbone.layers.1.attn.attn.in_proj_bias, auxiliary_head.transformer.backbone.layers.1.attn.attn.out_proj.weight, auxiliary_head.transformer.backbone.layers.1.attn.attn.out_proj.bias, auxiliary_head.transformer.backbone.layers.1.ln2.weight, auxiliary_head.transformer.backbone.layers.1.ln2.bias, auxiliary_head.transformer.backbone.layers.1.ffn.layers.0.0.weight, auxiliary_head.transformer.backbone.layers.1.ffn.layers.0.0.bias, auxiliary_head.transformer.backbone.layers.1.ffn.layers.1.weight, auxiliary_head.transformer.backbone.layers.1.ffn.layers.1.bias, auxiliary_head.transformer.backbone.layers.2.ln1.weight, auxiliary_head.transformer.backbone.layers.2.ln1.bias, auxiliary_head.transformer.backbone.layers.2.attn.attn.in_proj_weight, auxiliary_head.transformer.backbone.layers.2.attn.attn.in_proj_bias, auxiliary_head.transformer.backbone.layers.2.attn.attn.out_proj.weight, auxiliary_head.transformer.backbone.layers.2.attn.attn.out_proj.bias, auxiliary_head.transformer.backbone.layers.2.ln2.weight, auxiliary_head.transformer.backbone.layers.2.ln2.bias, auxiliary_head.transformer.backbone.layers.2.ffn.layers.0.0.weight, auxiliary_head.transformer.backbone.layers.2.ffn.layers.0.0.bias, auxiliary_head.transformer.backbone.layers.2.ffn.layers.1.weight, auxiliary_head.transformer.backbone.layers.2.ffn.layers.1.bias, auxiliary_head.transformer.backbone.layers.3.ln1.weight, auxiliary_head.transformer.backbone.layers.3.ln1.bias, auxiliary_head.transformer.backbone.layers.3.attn.attn.in_proj_weight, auxiliary_head.transformer.backbone.layers.3.attn.attn.in_proj_bias, auxiliary_head.transformer.backbone.layers.3.attn.attn.out_proj.weight, auxiliary_head.transformer.backbone.layers.3.attn.attn.out_proj.bias, auxiliary_head.transformer.backbone.layers.3.ln2.weight, auxiliary_head.transformer.backbone.layers.3.ln2.bias, auxiliary_head.transformer.backbone.layers.3.ffn.layers.0.0.weight, auxiliary_head.transformer.backbone.layers.3.ffn.layers.0.0.bias, auxiliary_head.transformer.backbone.layers.3.ffn.layers.1.weight, auxiliary_head.transformer.backbone.layers.3.ffn.layers.1.bias, auxiliary_head.transformer.backbone.layers.4.ln1.weight, auxiliary_head.transformer.backbone.layers.4.ln1.bias, auxiliary_head.transformer.backbone.layers.4.attn.attn.in_proj_weight, auxiliary_head.transformer.backbone.layers.4.attn.attn.in_proj_bias, auxiliary_head.transformer.backbone.layers.4.attn.attn.out_proj.weight, auxiliary_head.transformer.backbone.layers.4.attn.attn.out_proj.bias, auxiliary_head.transformer.backbone.layers.4.ln2.weight, auxiliary_head.transformer.backbone.layers.4.ln2.bias, auxiliary_head.transformer.backbone.layers.4.ffn.layers.0.0.weight, auxiliary_head.transformer.backbone.layers.4.ffn.layers.0.0.bias, auxiliary_head.transformer.backbone.layers.4.ffn.layers.1.weight, auxiliary_head.transformer.backbone.layers.4.ffn.layers.1.bias, auxiliary_head.transformer.backbone.layers.5.ln1.weight, auxiliary_head.transformer.backbone.layers.5.ln1.bias, auxiliary_head.transformer.backbone.layers.5.attn.attn.in_proj_weight, auxiliary_head.transformer.backbone.layers.5.attn.attn.in_proj_bias, auxiliary_head.transformer.backbone.layers.5.attn.attn.out_proj.weight, auxiliary_head.transformer.backbone.layers.5.attn.attn.out_proj.bias, auxiliary_head.transformer.backbone.layers.5.ln2.weight, auxiliary_head.transformer.backbone.layers.5.ln2.bias, auxiliary_head.transformer.backbone.layers.5.ffn.layers.0.0.weight, auxiliary_head.transformer.backbone.layers.5.ffn.layers.0.0.bias, auxiliary_head.transformer.backbone.layers.5.ffn.layers.1.weight, auxiliary_head.transformer.backbone.layers.5.ffn.layers.1.bias, auxiliary_head.transformer.backbone.layers.6.ln1.weight, auxiliary_head.transformer.backbone.layers.6.ln1.bias, auxiliary_head.transformer.backbone.layers.6.attn.attn.in_proj_weight, auxiliary_head.transformer.backbone.layers.6.attn.attn.in_proj_bias, auxiliary_head.transformer.backbone.layers.6.attn.attn.out_proj.weight, auxiliary_head.transformer.backbone.layers.6.attn.attn.out_proj.bias, auxiliary_head.transformer.backbone.layers.6.ln2.weight, auxiliary_head.transformer.backbone.layers.6.ln2.bias, auxiliary_head.transformer.backbone.layers.6.ffn.layers.0.0.weight, auxiliary_head.transformer.backbone.layers.6.ffn.layers.0.0.bias, auxiliary_head.transformer.backbone.layers.6.ffn.layers.1.weight, auxiliary_head.transformer.backbone.layers.6.ffn.layers.1.bias, auxiliary_head.transformer.backbone.layers.7.ln1.weight, auxiliary_head.transformer.backbone.layers.7.ln1.bias, auxiliary_head.transformer.backbone.layers.7.attn.attn.in_proj_weight, auxiliary_head.transformer.backbone.layers.7.attn.attn.in_proj_bias, auxiliary_head.transformer.backbone.layers.7.attn.attn.out_proj.weight, auxiliary_head.transformer.backbone.layers.7.attn.attn.out_proj.bias, auxiliary_head.transformer.backbone.layers.7.ln2.weight, auxiliary_head.transformer.backbone.layers.7.ln2.bias, auxiliary_head.transformer.backbone.layers.7.ffn.layers.0.0.weight, auxiliary_head.transformer.backbone.layers.7.ffn.layers.0.0.bias, auxiliary_head.transformer.backbone.layers.7.ffn.layers.1.weight, auxiliary_head.transformer.backbone.layers.7.ffn.layers.1.bias, auxiliary_head.transformer.backbone.layers.8.ln1.weight, auxiliary_head.transformer.backbone.layers.8.ln1.bias, auxiliary_head.transformer.backbone.layers.8.attn.attn.in_proj_weight, auxiliary_head.transformer.backbone.layers.8.attn.attn.in_proj_bias, auxiliary_head.transformer.backbone.layers.8.attn.attn.out_proj.weight, auxiliary_head.transformer.backbone.layers.8.attn.attn.out_proj.bias, auxiliary_head.transformer.backbone.layers.8.ln2.weight, auxiliary_head.transformer.backbone.layers.8.ln2.bias, auxiliary_head.transformer.backbone.layers.8.ffn.layers.0.0.weight, auxiliary_head.transformer.backbone.layers.8.ffn.layers.0.0.bias, auxiliary_head.transformer.backbone.layers.8.ffn.layers.1.weight, auxiliary_head.transformer.backbone.layers.8.ffn.layers.1.bias, auxiliary_head.transformer.backbone.layers.9.ln1.weight, auxiliary_head.transformer.backbone.layers.9.ln1.bias, auxiliary_head.transformer.backbone.layers.9.attn.attn.in_proj_weight, auxiliary_head.transformer.backbone.layers.9.attn.attn.in_proj_bias, auxiliary_head.transformer.backbone.layers.9.attn.attn.out_proj.weight, auxiliary_head.transformer.backbone.layers.9.attn.attn.out_proj.bias, auxiliary_head.transformer.backbone.layers.9.ln2.weight, auxiliary_head.transformer.backbone.layers.9.ln2.bias, auxiliary_head.transformer.backbone.layers.9.ffn.layers.0.0.weight, auxiliary_head.transformer.backbone.layers.9.ffn.layers.0.0.bias, auxiliary_head.transformer.backbone.layers.9.ffn.layers.1.weight, auxiliary_head.transformer.backbone.layers.9.ffn.layers.1.bias, auxiliary_head.transformer.backbone.layers.10.ln1.weight, auxiliary_head.transformer.backbone.layers.10.ln1.bias, auxiliary_head.transformer.backbone.layers.10.attn.attn.in_proj_weight, auxiliary_head.transformer.backbone.layers.10.attn.attn.in_proj_bias, auxiliary_head.transformer.backbone.layers.10.attn.attn.out_proj.weight, auxiliary_head.transformer.backbone.layers.10.attn.attn.out_proj.bias, auxiliary_head.transformer.backbone.layers.10.ln2.weight, auxiliary_head.transformer.backbone.layers.10.ln2.bias, auxiliary_head.transformer.backbone.layers.10.ffn.layers.0.0.weight, auxiliary_head.transformer.backbone.layers.10.ffn.layers.0.0.bias, auxiliary_head.transformer.backbone.layers.10.ffn.layers.1.weight, auxiliary_head.transformer.backbone.layers.10.ffn.layers.1.bias, auxiliary_head.transformer.backbone.layers.11.ln1.weight, auxiliary_head.transformer.backbone.layers.11.ln1.bias, auxiliary_head.transformer.backbone.layers.11.attn.attn.in_proj_weight, auxiliary_head.transformer.backbone.layers.11.attn.attn.in_proj_bias, auxiliary_head.transformer.backbone.layers.11.attn.attn.out_proj.weight, auxiliary_head.transformer.backbone.layers.11.attn.attn.out_proj.bias, auxiliary_head.transformer.backbone.layers.11.ln2.weight, auxiliary_head.transformer.backbone.layers.11.ln2.bias, auxiliary_head.transformer.backbone.layers.11.ffn.layers.0.0.weight, auxiliary_head.transformer.backbone.layers.11.ffn.layers.0.0.bias, auxiliary_head.transformer.backbone.layers.11.ffn.layers.1.weight, auxiliary_head.transformer.backbone.layers.11.ffn.layers.1.bias, auxiliary_head.transformer.backbone.ln1.weight, auxiliary_head.transformer.backbone.ln1.bias, auxiliary_head.transformer.decode_head.cls_emb, auxiliary_head.transformer.decode_head.layers.0.ln1.weight, auxiliary_head.transformer.decode_head.layers.0.ln1.bias, auxiliary_head.transformer.decode_head.layers.0.attn.attn.in_proj_weight, auxiliary_head.transformer.decode_head.layers.0.attn.attn.in_proj_bias, auxiliary_head.transformer.decode_head.layers.0.attn.attn.out_proj.weight, auxiliary_head.transformer.decode_head.layers.0.attn.attn.out_proj.bias, auxiliary_head.transformer.decode_head.layers.0.ln2.weight, auxiliary_head.transformer.decode_head.layers.0.ln2.bias, auxiliary_head.transformer.decode_head.layers.0.ffn.layers.0.0.weight, auxiliary_head.transformer.decode_head.layers.0.ffn.layers.0.0.bias, auxiliary_head.transformer.decode_head.layers.0.ffn.layers.1.weight, auxiliary_head.transformer.decode_head.layers.0.ffn.layers.1.bias, auxiliary_head.transformer.decode_head.layers.1.ln1.weight, auxiliary_head.transformer.decode_head.layers.1.ln1.bias, auxiliary_head.transformer.decode_head.layers.1.attn.attn.in_proj_weight, auxiliary_head.transformer.decode_head.layers.1.attn.attn.in_proj_bias, auxiliary_head.transformer.decode_head.layers.1.attn.attn.out_proj.weight, auxiliary_head.transformer.decode_head.layers.1.attn.attn.out_proj.bias, auxiliary_head.transformer.decode_head.layers.1.ln2.weight, auxiliary_head.transformer.decode_head.layers.1.ln2.bias, auxiliary_head.transformer.decode_head.layers.1.ffn.layers.0.0.weight, auxiliary_head.transformer.decode_head.layers.1.ffn.layers.0.0.bias, auxiliary_head.transformer.decode_head.layers.1.ffn.layers.1.weight, auxiliary_head.transformer.decode_head.layers.1.ffn.layers.1.bias, auxiliary_head.transformer.decode_head.dec_proj.weight, auxiliary_head.transformer.decode_head.dec_proj.bias, auxiliary_head.transformer.decode_head.patch_proj.weight, auxiliary_head.transformer.decode_head.classes_proj.weight, auxiliary_head.transformer.decode_head.decoder_norm.weight, auxiliary_head.transformer.decode_head.decoder_norm.bias, auxiliary_head.transformer.decode_head.mask_norm.weight, auxiliary_head.transformer.decode_head.mask_norm.bias, auxiliary_head.transformer.auxiliary_head.conv_seg.weight, auxiliary_head.transformer.auxiliary_head.conv_seg.bias, auxiliary_head.fc.0.weight, auxiliary_head.fc.0.bias, auxiliary_head.fc.2.weight, auxiliary_head.fc.2.bias

2022-12-10 02:15:18,582 - mmseg - INFO - Start running, host: richardalbr@n7, work_dir: /home/richardalbr/computer_vision/CLTforSemanticSegmentation/work_dirs/clt_segmentation
2022-12-10 02:15:18,582 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2022-12-10 02:15:18,582 - mmseg - INFO - workflow: [('train', 1)], max: 160000 iters
2022-12-10 02:15:18,583 - mmseg - INFO - Checkpoints will be saved to /home/richardalbr/computer_vision/CLTforSemanticSegmentation/work_dirs/clt_segmentation by HardDiskBackend.
DATASET mmsegmentation/mmseg/data/VOCdevkit/VOC2012
TRAIN {'type': 'PascalVOCDataset', 'data_root': '', 'img_dir': 'mmsegmentation/mmseg/data/VOCdevkit/VOC2012/JPEGImages/', 'ann_dir': 'mmsegmentation/mmseg/data/VOCdevkit/VOC2012/SegmentationClass/', 'split': 'mmsegmentation/mmseg/data/VOCdevkit/VOC2012/ImageSets/Segmentation/train.txt', 'pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'LoadAnnotations', 'reduce_zero_label': True}, {'type': 'Resize', 'img_scale': (2048, 512), 'ratio_range': (0.5, 2.0)}, {'type': 'RandomCrop', 'crop_size': (320, 320), 'cat_max_ratio': 0.75}, {'type': 'RandomFlip', 'prob': 0.5}, {'type': 'PhotoMetricDistortion'}, {'type': 'Normalize', 'mean': [127.5, 127.5, 127.5], 'std': [127.5, 127.5, 127.5], 'to_rgb': True}, {'type': 'Pad', 'size': (320, 320), 'pad_val': 0, 'seg_pad_val': 255}, {'type': 'DefaultFormatBundle'}, {'type': 'Collect', 'keys': ['img', 'gt_semantic_seg']}]}
VALIDATION {'type': 'PascalVOCDataset', 'data_root': '', 'img_dir': 'mmsegmentation/mmseg/data/VOCdevkit/VOC2012/JPEGImages/', 'ann_dir': 'mmsegmentation/mmseg/data/VOCdevkit/VOC2012/Annotations/', 'split': 'mmsegmentation/mmseg/data/VOCdevkit/VOC2012/ImageSets/Segmentation/val.txt', 'pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'MultiScaleFlipAug', 'img_scale': (2048, 512), 'flip': False, 'transforms': [{'type': 'Resize', 'keep_ratio': True}, {'type': 'RandomFlip'}, {'type': 'Normalize', 'mean': [127.5, 127.5, 127.5], 'std': [127.5, 127.5, 127.5], 'to_rgb': True}, {'type': 'ImageToTensor', 'keys': ['img']}, {'type': 'Collect', 'keys': ['img']}]}]}
load checkpoint from local path: /home/richardalbr/computer_vision/CLTforSemanticSegmentation/segmenter_vit-t_mask_8x1_512x512_160k_ade20k_20220105_151706-ffcf7509.pth
The model and loaded state dict do not match exactly

missing keys in source state_dict: auxiliary_head.conv_seg.weight, auxiliary_head.conv_seg.bias

Traceback (most recent call last):
  File "/home/richardalbr/computer_vision/CLTforSemanticSegmentation/train_test.py", line 111, in <module>
    main(args['model'], args['config'], args['dataset'], args['mode'])
  File "/home/richardalbr/computer_vision/CLTforSemanticSegmentation/train_test.py", line 70, in main
    train_segmentor(model, datasets, cfg, distributed=False, validate=True)
  File "/home/richardalbr/computer_vision/CLTforSemanticSegmentation/mmsegmentation/mmseg/apis/train.py", line 194, in train_segmentor
    runner.run(data_loaders, cfg.workflow)
  File "/home/richardalbr/computer_vision/cvvenv/lib/python3.9/site-packages/mmcv/runner/iter_based_runner.py", line 144, in run
    iter_runner(iter_loaders[i], **kwargs)
  File "/home/richardalbr/computer_vision/cvvenv/lib/python3.9/site-packages/mmcv/runner/iter_based_runner.py", line 64, in train
    outputs = self.model.train_step(data_batch, self.optimizer, **kwargs)
  File "/home/richardalbr/computer_vision/cvvenv/lib/python3.9/site-packages/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/home/richardalbr/computer_vision/CLTforSemanticSegmentation/mmsegmentation/mmseg/models/segmentors/base.py", line 138, in train_step
    losses = self(**data_batch)
  File "/home/richardalbr/computer_vision/cvvenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/richardalbr/computer_vision/cvvenv/lib/python3.9/site-packages/mmcv/runner/fp16_utils.py", line 116, in new_func
    return old_func(*args, **kwargs)
  File "/home/richardalbr/computer_vision/CLTforSemanticSegmentation/mmsegmentation/mmseg/models/segmentors/base.py", line 108, in forward
    return self.forward_train(img, img_metas, **kwargs)
  File "/home/richardalbr/computer_vision/CLTforSemanticSegmentation/mmsegmentation/mmseg/models/segmentors/encoder_decoder.py", line 149, in forward_train
    loss_aux = self._auxiliary_head_forward_train(
  File "/home/richardalbr/computer_vision/CLTforSemanticSegmentation/mmsegmentation/mmseg/models/segmentors/encoder_decoder.py", line 111, in _auxiliary_head_forward_train
    loss_aux = self.auxiliary_head.forward_train(
  File "/home/richardalbr/computer_vision/CLTforSemanticSegmentation/mmsegmentation/mmseg/models/decode_heads/decode_head.py", line 232, in forward_train
    seg_logits = self(inputs)
  File "/home/richardalbr/computer_vision/cvvenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/richardalbr/computer_vision/CLTforSemanticSegmentation/mmsegmentation/mmseg/models/decode_heads/projection_head.py", line 45, in forward
    output = self._forward_feature(inputs)
  File "/home/richardalbr/computer_vision/CLTforSemanticSegmentation/mmsegmentation/mmseg/models/decode_heads/projection_head.py", line 39, in _forward_feature
    logits = self.transformer(x)
  File "/home/richardalbr/computer_vision/cvvenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/richardalbr/computer_vision/cvvenv/lib/python3.9/site-packages/mmcv/runner/fp16_utils.py", line 116, in new_func
    return old_func(*args, **kwargs)
TypeError: forward() missing 1 required positional argument: 'img_metas'
